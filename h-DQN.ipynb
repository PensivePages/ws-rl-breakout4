{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"h-DQN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZzZqWYiN9ReQ","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import cv2\n","import random\n","import matplotlib.pyplot as plt\n","import gym\n","#import universe # register the universe environments\n","from collections import defaultdict #for Q ie: state Q values store\n","from collections import namedtuple #for transistions store ie: memory, buffer, ReplayMemory"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zjkUtvrrQKBe","colab_type":"code","outputId":"47f93e9b-d2cc-470b-c776-f49c315cabd7","executionInfo":{"status":"ok","timestamp":1570483182705,"user_tz":300,"elapsed":4795,"user":{"displayName":"Pensive","photoUrl":"","userId":"04437951639433759578"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["import atari_py as ap\n","game_list = ap.list_games()\n","print(sorted(game_list))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["['adventure', 'air_raid', 'alien', 'amidar', 'assault', 'asterix', 'asteroids', 'atlantis', 'bank_heist', 'battle_zone', 'beam_rider', 'berzerk', 'bowling', 'boxing', 'breakout', 'carnival', 'centipede', 'chopper_command', 'crazy_climber', 'defender', 'demon_attack', 'double_dunk', 'elevator_action', 'enduro', 'fishing_derby', 'freeway', 'frostbite', 'gopher', 'gravitar', 'hero', 'ice_hockey', 'jamesbond', 'journey_escape', 'kaboom', 'kangaroo', 'krull', 'kung_fu_master', 'montezuma_revenge', 'ms_pacman', 'name_this_game', 'phoenix', 'pitfall', 'pong', 'pooyan', 'private_eye', 'qbert', 'riverraid', 'road_runner', 'robotank', 'seaquest', 'skiing', 'solaris', 'space_invaders', 'star_gunner', 'tennis', 'time_pilot', 'tutankham', 'up_n_down', 'venture', 'video_pinball', 'wizard_of_wor', 'yars_revenge', 'zaxxon']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"10QkYDLcRsZI","colab_type":"code","outputId":"c3137167-eb93-44b5-9bc0-9ff1408fd476","executionInfo":{"status":"ok","timestamp":1570483235030,"user_tz":300,"elapsed":57106,"user":{"displayName":"Pensive","photoUrl":"","userId":"04437951639433759578"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!git clone https://github.com/openai/universe.git\n","!cd universe\n","!pip install -e .\n","#!pip install 'gym[atari]'\n","!pip install universe\n","import universe # register the universe environments"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'universe'...\n","remote: Enumerating objects: 1476, done.\u001b[K\n","remote: Total 1476 (delta 0), reused 0 (delta 0), pack-reused 1476\u001b[K\n","Receiving objects: 100% (1476/1476), 1.58 MiB | 4.23 MiB/s, done.\n","Resolving deltas: 100% (938/938), done.\n","\u001b[31mERROR: File \"setup.py\" not found. Directory cannot be installed in editable mode: /content\u001b[0m\n","Collecting universe\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/5e/26ee56c16cdc83a07a3251e3425856f2ade84928de55b3fa11590ce2b912/universe-0.21.3.tar.gz (136kB)\n","\u001b[K     |████████████████████████████████| 143kB 4.9MB/s \n","\u001b[?25hCollecting autobahn>=0.16.0 (from universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/1e/619ab12c3cf4dfd251dcb6cbca9581fbb2e648bf80f68d7be4b4b6b99337/autobahn-19.10.1-py2.py3-none-any.whl (771kB)\n","\u001b[K     |████████████████████████████████| 778kB 22.7MB/s \n","\u001b[?25hCollecting docker-py==1.10.3 (from universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/10/5234c4fa9710a04184586d877d028a51dc395a1867c3d4317438514dff77/docker_py-1.10.3-py2.py3-none-any.whl (48kB)\n","\u001b[K     |████████████████████████████████| 51kB 19.0MB/s \n","\u001b[?25hCollecting docker-pycreds==0.2.1 (from universe)\n","  Downloading https://files.pythonhosted.org/packages/ff/ec/14e976222468249cc50df4a031e5922581747b52dc96a0b1d5d73fee8f0e/docker_pycreds-0.2.1-py2.py3-none-any.whl\n","Collecting fastzbarlight>=0.0.13 (from universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/f6/b46fe453ab12ac08c6a15808d402963819f26d4bd9daa5a899f6a3bae22d/fastzbarlight-0.0.14.tar.gz (728kB)\n","\u001b[K     |████████████████████████████████| 737kB 39.8MB/s \n","\u001b[?25hCollecting go-vncdriver>=0.4.8 (from universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/29/5a3036974ac2d86991a806d0136c1437b851ec61481694b8b6adeafde81b/go_vncdriver-0.4.19.tar.gz (638kB)\n","\u001b[K     |████████████████████████████████| 645kB 32.5MB/s \n","\u001b[?25hCollecting gym<0.8,>=0.7 (from universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/af/15bc80c507108cf6de27fd356bb5939ba64b437610412e48b186ee0dedfd/gym-0.7.4.tar.gz (152kB)\n","\u001b[K     |████████████████████████████████| 153kB 34.2MB/s \n","\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from universe) (4.3.0)\n","Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from universe) (3.13)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from universe) (1.12.0)\n","Collecting twisted>=16.5.0 (from universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/49/eb654da38b15285d1f594933eefff36ce03106356197dba28ee8f5721a79/Twisted-19.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.1MB 32.3MB/s \n","\u001b[?25hCollecting ujson>=1.35 (from universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n","\u001b[K     |████████████████████████████████| 194kB 38.6MB/s \n","\u001b[?25hCollecting cryptography>=2.7 (from autobahn>=0.16.0->universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 34.9MB/s \n","\u001b[?25hCollecting txaio>=18.8.1 (from autobahn>=0.16.0->universe)\n","  Downloading https://files.pythonhosted.org/packages/e9/6d/e1a6f7835cde86728e5bb1f577be9b2d7d273fdb33c286e70b087d418ded/txaio-18.8.1-py2.py3-none-any.whl\n","Collecting websocket-client>=0.32.0 (from docker-py==1.10.3->universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n","\u001b[K     |████████████████████████████████| 204kB 43.5MB/s \n","\u001b[?25hCollecting requests<2.11,>=2.5.2 (from docker-py==1.10.3->universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/b4/63d99ba8e189c47d906b43bae18af4396e336f2b1bfec86af31efe2d2cb8/requests-2.10.0-py2.py3-none-any.whl (506kB)\n","\u001b[K     |████████████████████████████████| 512kB 44.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from go-vncdriver>=0.4.8->universe) (1.16.5)\n","Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<0.8,>=0.7->universe) (1.4.4)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow>=3.3.0->universe) (0.46)\n","Collecting incremental>=16.10.1 (from twisted>=16.5.0->universe)\n","  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n","Collecting hyperlink>=17.1.1 (from twisted>=16.5.0->universe)\n","  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n","Collecting zope.interface>=4.4.2 (from twisted>=16.5.0->universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/17/1d198a6aaa9aa4590862fe3d3a2ed7dd808050cab4eebe8a2f2f813c1376/zope.interface-4.6.0-cp36-cp36m-manylinux1_x86_64.whl (167kB)\n","\u001b[K     |████████████████████████████████| 174kB 36.8MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from twisted>=16.5.0->universe) (19.1.0)\n","Collecting Automat>=0.3.0 (from twisted>=16.5.0->universe)\n","  Downloading https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl\n","Collecting constantly>=15.1 (from twisted>=16.5.0->universe)\n","  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n","Collecting PyHamcrest>=1.9.0 (from twisted>=16.5.0->universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n","\u001b[K     |████████████████████████████████| 61kB 23.9MB/s \n","\u001b[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.7->autobahn>=0.16.0->universe) (1.12.3)\n","Collecting asn1crypto>=0.21.0 (from cryptography>=2.7->autobahn>=0.16.0->universe)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/c3/be192e56ab85a7b2439caf83049b9936292aaf0398038a2fe28716f18951/asn1crypto-1.0.1-py2.py3-none-any.whl (103kB)\n","\u001b[K     |████████████████████████████████| 112kB 47.8MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym<0.8,>=0.7->universe) (0.16.0)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->twisted>=16.5.0->universe) (2.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.4.2->twisted>=16.5.0->universe) (41.2.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.7->autobahn>=0.16.0->universe) (2.19)\n","Building wheels for collected packages: universe, fastzbarlight, go-vncdriver, gym, ujson\n","  Building wheel for universe (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for universe: filename=universe-0.21.3-cp36-none-any.whl size=167584 sha256=1cbb5130f076115508ac8e9b2a72d2b08ba966e78b14eef5074c3bed1ab1813c\n","  Stored in directory: /root/.cache/pip/wheels/65/71/6f/d7604d1848e86ea29abcbd111086a74de6cc5506bfae8a4619\n","  Building wheel for fastzbarlight (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fastzbarlight: filename=fastzbarlight-0.0.14-cp36-cp36m-linux_x86_64.whl size=1084106 sha256=a456556a6c7c7fb697f10c7e62783bb88b822455873542cbe229b3d1e3d9f0d7\n","  Stored in directory: /root/.cache/pip/wheels/66/49/ce/1ad00cfb42897cdcdedf5239c05609ab58dbcadb3d3399d42a\n","  Building wheel for go-vncdriver (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for go-vncdriver\u001b[0m\n","\u001b[?25h  Running setup.py clean for go-vncdriver\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.7.4-cp36-none-any.whl size=204693 sha256=c6e9695da3f84d6a8198a2c3709dff11e03b5eb1f70b59c15e0067a1072fdd63\n","  Stored in directory: /root/.cache/pip/wheels/e2/0e/9a/758e93d957d3665bdeafb3e04837e063fff8279151f34d651d\n","  Building wheel for ujson (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ujson: filename=ujson-1.35-cp36-cp36m-linux_x86_64.whl size=68036 sha256=1ade859bc729badd3c72ae7efb2dbb76ebf909283a88dfcbd62f285379699e36\n","  Stored in directory: /root/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n","Successfully built universe fastzbarlight gym ujson\n","Failed to build go-vncdriver\n","\u001b[31mERROR: tweepy 3.6.0 has requirement requests>=2.11.1, but you'll have requests 2.10.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tfds-nightly 1.2.0.dev201909050105 has requirement requests>=2.19.0, but you'll have requests 2.10.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: stable-baselines 2.2.1 has requirement gym[atari,classic_control]>=0.10.9, but you'll have gym 0.7.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: spacy 2.1.8 has requirement requests<3.0.0,>=2.13.0, but you'll have requests 2.10.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.21.0, but you'll have requests 2.10.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-api-core 1.14.2 has requirement requests<3.0.0dev,>=2.18.0, but you'll have requests 2.10.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: dopamine-rl 1.0.5 has requirement gym>=0.10.5, but you'll have gym 0.7.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","Installing collected packages: asn1crypto, cryptography, txaio, autobahn, websocket-client, docker-pycreds, requests, docker-py, fastzbarlight, go-vncdriver, gym, incremental, hyperlink, zope.interface, Automat, constantly, PyHamcrest, twisted, ujson, universe\n","  Found existing installation: requests 2.21.0\n","    Uninstalling requests-2.21.0:\n","      Successfully uninstalled requests-2.21.0\n","  Running setup.py install for go-vncdriver ... \u001b[?25l\u001b[?25herror\n","\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-hzp5_k7b/go-vncdriver/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-hzp5_k7b/go-vncdriver/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-mv3srf8m/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HG3iepVJTumG","colab_type":"code","colab":{}},"source":["#env = gym.make('MontezumaRevengeNoFrameskip-v4') #play env\n","# env = gym.make('MontezumaRevenge-v0')\n","# env.reset()\n","#env.render()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLK4vHRKGA9D","colab_type":"code","colab":{}},"source":["# custom weights initialization\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)\n","\n","# conv helper \n","def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True, init_zero_weights=False):\n","    \"\"\"Creates a convolutional layer, with optional batch normalization.\n","    \"\"\"\n","    layers = []\n","    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n","    if init_zero_weights:\n","        conv_layer.weight.data = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n","    layers.append(conv_layer)\n","\n","    if batch_norm:\n","        layers.append(nn.BatchNorm2d(out_channels))\n","    return nn.Sequential(*layers)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hT9-V3NNq0w","colab_type":"code","colab":{}},"source":["class Meta_Controller(nn.Module):\n","    # Meta-controller predicts the value of a goal\n","    def __init__(self, MC_conv_out, MC_hidden_size, MC_number_of_goals, ngpu):\n","        super(Meta_Controller, self).__init__()\n","        self.ngpu = ngpu\n","        # conv(channels_in, channels_out, kernel, stride, padding)\n","        self.conv1 = conv(1, 32, 8, 4, 0) #all 3 convs set to paper figure 5b Q1 example\n","        self.conv2 = conv(32, 64, 4, 2, 0)\n","        self.conv3 = conv(64, 64, 3, 1, 0)\n","        self.projection = nn.Linear(MC_conv_out, MC_hidden_size) #set to h=512 via paper figure 5b\n","        self.output = nn.Linear(MC_hidden_size, MC_number_of_goals)\n","        self.nonLinearity = torch.nn.ReLU()\n","\n","    def forward(self, x):\n","        #print(\"inside Meta_Controller.forward\")\n","        x = self.nonLinearity(self.conv1(x))\n","        x = self.nonLinearity(self.conv2(x))\n","        x = self.nonLinearity(self.conv3(x))\n","        x = x.view(1, -1) # flatten\n","        x = self.nonLinearity(self.projection(x))\n","        x = self.output(x)\n","        return x[0]\n","\n","class Controller(nn.Module):\n","    # Controller predicts the value of a action\n","    def __init__(self, C_conv_out, C_hidden_size, C_number_of_actions, ngpu):\n","        super(Controller, self).__init__()\n","        self.ngpu = ngpu\n","        # conv(channels_in, channels_out, kernel, stride, padding)\n","        self.conv1 = conv(1, 32, 8, 4, 0) #all 3 convs set to paper figure 5b\n","        self.conv2 = conv(32, 64, 4, 2, 0)\n","        self.conv3 = conv(64, 64, 3, 1, 0)\n","        self.projection = nn.Linear(C_conv_out, C_hidden_size) #set to h=512 via paper figure 5b\n","        self.output = nn.Linear(C_hidden_size, C_number_of_actions)\n","        self.nonLinearity = torch.nn.ReLU()\n","\n","    def forward(self, x):\n","        #print(\"inside Controller.forward\")\n","        x = self.nonLinearity(self.conv1(x))\n","        x = self.nonLinearity(self.conv2(x))\n","        x = self.nonLinearity(self.conv3(x))\n","        x = x.view(1, -1) # flatten\n","        x = self.nonLinearity(self.projection(x))\n","        x = self.output(x)\n","        return x[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJldHbmnN4Ju","colab_type":"code","colab":{}},"source":["#memory code from AISC RL Workshop\n","class ReplayMemory(object):\n","    '''\n","        A simple memory for storing episodes where each episodes \n","        is a names tuple with (state, action, next_state, reward, done)\n","    '''\n","\n","    def __init__(self, capacity):\n","        '''\n","          Initialize memory of size capacity\n","          Input: Capacity : int \n","                      size of the memory\n","\n","          output: initialized ReplayMemory object\n","        '''\n","        self.capacity = capacity\n","        self.memory = []\n","        self.position = 0\n","\n","    def push(self, meta, *args): # meta=True or False to pick storage (MC/C)\n","        '''\n","          input *args : list  *args is list for transition \n","          [state, action, next_state, reward, done] and add\n","          transition to memory.\n","          Returns : None\n","        '''\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None) #give it space in memory to set\n","        #self.memory[self.position] = Transition(*args) #original\n","        if meta:\n","            self.memory[self.position] = D2(*args)\n","        else:\n","            self.memory[self.position] = D1(*args)\n","        self.position = (self.position + 1) % self.capacity \n","        #^ mod makes it loop around once it passed 10k\n","\n","    def sample(self, batch_size, meta): # meta=True or False to pick storage (MC/C)\n","        '''\n","          Randomly sample transitions from memory\n","          Input batch_size : int\n","                  numer of transition to sample\n","          Output:  namedtuple\n","                    Namedtupe with each field contains a list of data points\n","\n","        '''\n","        batch = random.sample(self.memory, batch_size)\n","        #return Transition(*zip(*batch)) #original\n","        if meta:\n","            return D2(*zip(*batch))\n","        else:\n","            return D1(*zip(*batch))\n","\n","\n","    def __len__(self):\n","        '''\n","            returns current size of memory\n","        '''\n","        return len(self.memory)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBf8pFtkN6iJ","colab_type":"code","colab":{}},"source":["class Agent():\n","\n","    def __init__(self, env, MC_buffer, C_buffer,\\\n","                 learning_rate, gamma,\\\n","                 exploration_param2, exploration_param1,\\\n","                 max_goals_to_try, batch_size, extrinsic_tries_before_eval, conv_stack='stacked'): #conv_stack = stacked' for 4, 1, 84, 84, or 'side_by_side' for 1, 1, 84, 336\n","        self.new_session = True\n","        self.env = env\n","        self.learning_rate = learning_rate\n","        self.MC_buffer = MC_buffer\n","        self.C_buffer = C_buffer\n","        self.gamma = gamma # \"discount rate\"\n","        self.ep2 = exploration_param2 #MC, meta-controller\n","        self.ep1 = exploration_param1 #C, controller\n","        #self.state = None #input\n","        self.done = False\n","        self.terminated = False\n","        self.max_goals_to_try = max_goals_to_try\n","        self.num_goals_tried = 0\n","        self.Q1 = None\n","        self.Q2 = None\n","        self.batch_size = batch_size\n","        self.intrinsic_reward = 0\n","        self.intrinsic_success_count = 0\n","        self.intrinsic_total = 0\n","        self.extrinsic_success_count = 0\n","        self.total_sessions = 0\n","        self.eval_tries = extrinsic_tries_before_eval\n","        self.goals = []\n","        self.actions = list(range(self.env.action_space.n))\n","        self.num_actions = 0\n","        self.session_success = False\n","        self.session_success_list = []\n","        self.session_success_list_unique_count = []\n","        self.ss_count = 0\n","        self.ml_success = 0 #for middel_ladder threachold for reducing explore\n","        self.lives = 0\n","        self.max_steps = 5000\n","        self.max_score = 400\n","        self.max_intrinsic_score = 6\n","        self.score = 0\n","        self.track_actions = np.zeros(self.env.action_space.n)\n","        self.track_goals = np.zeros(6)\n","        # reset memory\n","        C_buffer.__init__(1000000)\n","        MC_buffer.__init__(50000)\n","\n","    def show_state_image(state, conv_stack='stacked'):\n","        if conv_stack=='stacked':\n","            observationCheck = state[0].view(84,84).numpy() # 4, 1, 84, 84\n","        else:\n","            observationcheck = state[:, :, :, :84].view(84,84).numpy() # 1, 1, 84, 336\n","        print(\"check goal (%s) color: \"%(goal_name) + str(np.array(observationCheck).shape))\n","        plt.imshow(np.array(np.squeeze(observationCheck)))\n","        plt.show()\n","\n","    def softmax(self, x):\n","        return (np.exp(x - np.amax(x)) / np.sum(np.exp(x - np.amax(x))))\n","\n","    def change_pixels(self, state, center_pixel: list, padding: int): #state tensor is size (N, C, W, H)\n","        staring_row = center_pixel[0] - padding\n","        starting_column = center_pixel[1] - padding\n","        side = padding * 2 + 1\n","        i = staring_row\n","        while i < staring_row + side:\n","            j = starting_column\n","            while j < starting_column + side:\n","                if i == center_pixel[0] and j == center_pixel[1]:\n","                    pass\n","                else:\n","                    state[0][0][i][j] = 16\n","                j += 1\n","            i += 1\n","        return state, center_pixel\n","    \n","    def create_goals(self, state):\n","        #goal_template = torch.ones_like(state)\n","        # goal 1 (key): change pixels: center: row: 30, column: 9, padding: 1?\n","        key = lambda state: self.change_pixels(state, [30, 7], 3)\n","        # goal 2 (middle-ladder): change pixels: center: row: 35, column: 41, padding: _?\n","        middle_ladder = lambda state: self.change_pixels(state, [25, 41], 3)\n","        # goal 3 (left-ladder): change pixels: center: row: 59, column: 17, padding: _?\n","        left_ladder = lambda state: self.change_pixels(state, [55, 13], 3)\n","        # goal 4 (right-ladder): change pixels: center: row: 59, column: 71, padding: _?\n","        right_ladder = lambda state: self.change_pixels(state, [59, 71], 3)\n","        # goal 5 (left-door): change pixels: center: row: 10, column: 10, padding: _?\n","        left_door = lambda state: self.change_pixels(state, [12, 14], 3)\n","        # goal 6 (right-door): change pixels: center: row: 10, column: 76, padding: _?\n","        right_door = lambda state: self.change_pixels(state, [12, 69], 3)\n","        self.goals = [key, middle_ladder, left_ladder, right_ladder, left_door, right_door]\n","        self.goals_names = ['key', 'middle_ladder', 'left_ladder', 'right_ladder', 'left_door', 'right_door']\n","\n","    def init_Q(self, env): #not actually used atm\n","        # create Q value store if none\n","        self.Q1 = defaultdict(lambda: np.zeros(env.action_space.n))\n","        self.Q2 = defaultdict(lambda: np.zeros(len(goals)))\n","\n","    def init_session(self):\n","        # reset state to init\n","        # self.env = utils.wrap_env(gym.make('MontezumaRevenge-v4'))\n","        state = self.env.reset()\n","        self.done = False\n","        # reset time_step to init\n","        time_step = 0 #not in use atm\n","        # create goals\n","        if not self.goals:\n","            self.create_goals(state)\n","        return state, time_step\n","\n","    # preprocess() from: http://www.pinchofintelligence.com/openai-gym-part-3-playing-space-invaders-deep-reinforcement-learning/\n","    def preprocess(self, observation):\n","        observation = cv2.cvtColor(cv2.resize(observation, (84, 110)), cv2.COLOR_BGR2GRAY)\n","        observation = observation[26:110,:]\n","        #ret, observation = cv2.threshold(observation,1,255,cv2.THRESH_BINARY)\n","        ret, observation = cv2.threshold(observation,20,255,cv2.THRESH_TRUNC)\n","        #ret, observation = cv2.threshold(observation,0,255,cv2.THRESH_TOZERO)\n","        return np.reshape(observation,(1,84,84))\n","\n","    def format_state(self, state, next_state=None, test=False, first=False):\n","        # init starting image state with 3 previous void images states\n","        if self.new_session:\n","            state = self.preprocess(state)\n","            state = torch.from_numpy(state).unsqueeze(dim=0).type('torch.FloatTensor')\n","            ph_state = torch.zeros_like(state)\n","            #--for 4, 1, 84, 84--\n","            if conv_stack=='stacked':\n","                state = torch.cat((state, ph_state), dim=0)\n","                state = torch.cat((state, ph_state), dim=0)\n","                state = torch.cat((state, ph_state), dim=0)\n","            #--for 1, 1, 84, 336--\n","            else:\n","                state = torch.cat((state, ph_state), dim=-1)\n","                state = torch.cat((state, ph_state), dim=-1)\n","                state = torch.cat((state, ph_state), dim=-1)\n","            #some test code from preprocess() source\n","            if first:\n","                action0 = 0  # do nothing\n","                observation0, reward0, terminal, info = self.env.step(action0)\n","                print(\"action space: \", env.action_space)\n","                print(\"observation space: \", env.observation_space)\n","                print(\"reward0: \", reward0)\n","                print(\"terminal: \", terminal)\n","                print(\"info: \", info)\n","                print(\"Before processing: \" + str(np.array(observation0).shape))\n","                plt.imshow(np.array(observation0))\n","                plt.show()\n","                observation0 = self.preprocess(observation0)\n","                print(\"After processing: \" + str(np.array(observation0).shape))\n","                plt.imshow(np.array(np.squeeze(observation0)))\n","                plt.show()\n","                print(\"new observation space: \", env.observation_space)\n","                #brain.setInitState(observation0) #test model not used\n","                #brain.currentState = np.squeeze(brain.currentState) #test model not used\n","        else: #advance state image set to be 'next state, previous, previous, previous'\n","            next_state = self.preprocess(next_state)\n","            next_state = torch.from_numpy(next_state).unsqueeze(dim=0).type('torch.FloatTensor')\n","            state = self.update_state(state, next_state) #output = 'next state, previous, previous, previous'\n","        if test: #for viewing state image matricies\n","            print(\"state size (shape): \", state.size())\n","            batch = state\n","            bottom_label = list(range(0, 83))\n","            for image in batch:\n","                for channel in image:\n","                    for i, grid in enumerate(channel):\n","                        print(\"Grid %s: \"%(i), grid)\n","                        print(\"label  :        \", bottom_label)\n","                    break #break after 1\n","            raise NotImplementedError\n","        \n","        return state.to(device)\n","    \n","    def update_state(self, state, next_state):\n","        #--for 4, 1, 84, 84--\n","        if conv_stack=='stacked': #*arg not passed in atm, hard coded\n","            state[1:] = state[:-1]\n","            state[0] = next_state\n","        #--for 1, 1, 84, 336--\n","        else:\n","            state[:, :, :, 84:] = state[:, :, :, :252]\n","            state[:, :, :, :84] = next_state\n","        return state\n","\n","    def critic(self, state, action, goal, goal_name):\n","        #-execute action in env\n","        #self.env.render(mode='ansi')\n","        #env.reset()\n","        next_state, extrinsic_reward, done, info = env.step(action)\n","        self.max_steps += 1\n","        #-format next_state\n","        next_state = self.format_state(state, next_state)\n","        next_state, goal_pixel = goal(state) # put goal on current state image\n","        #-check color and goal\n","        #show_state_image(next_state)\n","        #-get intrinsic reward\n","        #if next_state[0][0][goal_pixel[0]][goal_pixel[1]] == 255:\n","        if next_state[0][0][goal_pixel[0]][goal_pixel[1]] == 20:\n","            #print(\"Got the (ง°ل͜°)ง %s!\"%(goal_name))\n","            #--show goal images\n","            #show_state_image(next_state)\n","            self.session_success_list.append(goal_name)\n","            if goal_name == 'key':\n","                print(\"!!!!!!!!!!!!!!!!!!!!Got the [̲̅$̲̅(̲̅ ͡° ͜ʖ ͡°̲̅)̲̅$̲̅]  %s  [̲̅$̲̅(̲̅ ͡° ͜ʖ ͡°̲̅)̲̅$̲̅] !!!!!!!!!!!!!!!!!!!!\"%(goal_name))\n","            return extrinsic_reward, 1, next_state, done #intrinsic_reward = 1\n","        else:\n","            if info['ale.lives'] < self.lives:\n","                self.lives -= 1\n","                return extrinsic_reward, -10, next_state, done #intrinsic_reward = -10 for lost life (not in mentioned in paper)\n","            else:\n","                return extrinsic_reward, 0, next_state, done #intrinsic_reward = 0\n","\n","    def select_direction(self, choices, exploration_param): #select goal, or action, greedy vs explore\n","        random = np.random.uniform(size=1)[0]\n","        if random <= exploration_param: #explore\n","            return np.random.choice(np.arange(len(choices)))\n","        else: #greedy\n","            return np.argmax(choices.detach().numpy())\n","\n","    def train(self):\n","        first = True\n","        # if self.Q1 == None:\n","        #     self.init_Q(env)\n","        bQn = 0 #count for batch to break for training\n","        while self.num_goals_tried < self.max_goals_to_try:\n","            if self.new_session == True:\n","                # reset session\n","                state, time_step = self.init_session()\n","                state = self.format_state(state, first=first, test=False)\n","                self.new_session = False\n","                first = False\n","                self.session_success = False\n","                self.session_success_list = []\n","                self.lives = 6\n","                self.max_steps = 500\n","                self.score = 0\n","                self.ss_count = 0\n","                self.intrinsic_success_count = 0\n","                self.intrinsic_total = 0\n","                self.done = False\n","            Q2_prediction = MC_pred(state) # set Q values for goal at given state\n","            i_goal = self.select_direction(Q2_prediction, self.ep2) # select goal intex\n","            goal_name = self.goals_names[i_goal]\n","            #print(\"The current goal is: \", goal_name)\n","            #print(\"goal is (☞ﾟヮﾟ)☞  %s  ☜(ﾟヮﾟ☜)\"%(goal_name))\n","            goal = self.goals[i_goal] # select goal function\n","            state_goal, goal_pixel = goal(state) # put goal on current state image\n","            self.track_goals[i_goal] += 1\n","            #-show goal\n","            #show_state_image(state_goal)\n","            self.intrinsic_reward = 0\n","            while self.intrinsic_reward == 0 and not self.done or self.max_steps < 500:\n","                Q1_prediction = C_pred(state_goal) # set Q values for action at given state and goal\n","                i_action = self.select_direction(Q1_prediction, self.ep1) # select action intex\n","                action = self.actions[i_action] # select action from list of actions by index\n","                #self.track_actions[i_action] += 1\n","                self.num_actions += 1\n","                extrinsic_reward, self.intrinsic_reward, next_state, self.done = self.critic(state, action, goal, goal_name) #get rewards and state\n","                self.score += (extrinsic_reward + self.intrinsic_reward)\n","                #-store transitions\n","                C_buffer.push(False, state_goal, action, self.intrinsic_reward, next_state) #store controller transition\n","                MC_buffer.push(True, state, i_goal, extrinsic_reward, next_state) #store meta-controller transition\n","                #-update state\n","                state = next_state\n","                #-manage batch NN update cycle\n","                bQn += 1\n","                if bQn % 50000 == 0 and bQn != 0:\n","                    sample = C_buffer.sample(self.batch_size, False)\n","                    self.update_Q1(sample.state, sample.action, sample.next_state, sample.reward)\n","                    # sample = MC_buffer.sample(self.batch_size, True)\n","                    # self.update_Q2(sample.state, sample.goal, sample.next_state, sample.reward)\n","                #-track intrinsic reward success and total\n","                if self.intrinsic_reward > 0:\n","                    #print(\"last goal achieved in %s actions!\"%(self.num_actions))\n","                    self.num_actions = 0\n","                    self.intrinsic_success_count += 1\n","                    #print(\"self.intrinsic_success_count: \", self.intrinsic_success_count)\n","                    self.session_success = True\n","                    #utils.show_video()\n","                self.intrinsic_total += 1\n","                #-track extrinsic reward success\n","                if extrinsic_reward > 0:\n","                    self.extrinsic_success_count += 1\n","            #-track number of goals tried so far\n","            self.num_goals_tried += 1\n","            #-when done reset session\n","            if self.done or self.max_steps == 500:\n","                #-track total sessions\n","                self.total_sessions += 1\n","                #-new session\n","                self.new_session = True\n","                print(\"----done----\")\n","                #print(\"self.intrinsic_success_count: \", self.intrinsic_success_count)\n","                #print(\"self.extrinsic_success_count: \", self.extrinsic_success_count)\n","                print(\"self.total_sessions: \", self.total_sessions)\n","                print(\"Session ending score: \", self.score)\n","                #print(\"Action selection percents: \", self.softmax(self.track_actions))\n","                print(\"Goal names: \", self.goals_names)\n","                print(\"Goal selection percents: \", self.softmax(self.track_goals))\n","                self.env.close()\n","                # utils.show_video()\n","                if self.session_success:\n","                    print(\"Success in this video are: \", self.session_success_list)\n","                    self.session_success_list_unique_count = []\n","                    for name in self.session_success_list:\n","                        if not name in self.session_success_list_unique_count:\n","                            self.session_success_list_unique_count.append(name)\n","                        if 'middle_ladder' in self.session_success_list_unique_count:\n","                            self.ml_success += 1\n","                    self.ss_count = len(self.session_success_list_unique_count)\n","                    #stop middle_ladder from reducing explore beyond .6\n","                    if 'middle_ladder' in self.session_success_list_unique_count and self.ml_success > 40: #for middel_ladder threachold for reducing explore\n","                        self.ss_count -= 1\n","                    #utils.show_video()\n","                if self.ss_count > 0:\n","                    print(\"reduce by: \", (.01 * self.ss_count))\n","                    self.ep1 = self.ep1 - (.01 * self.ss_count)\n","                if self.ep1 < .1:\n","                    self.ep1 = .1 #cap minimum exploration at .1\n","                print(\"New action exploration is %s\"%(self.ep1))\n","                print(\"------------\")\n","            if self.num_goals_tried%self.eval_tries == 0:\n","                sample = MC_buffer.sample(self.batch_size, True)\n","                self.update_Q2(sample.state, sample.goal, sample.next_state, sample.reward)\n","                #-how often extrinsic goal was reached\n","                self.ep2 = self.ep2 - (.01 * self.extrinsic_success_count)\n","                if self.ep2 < .1:\n","                    self.ep2 = .1 #cap minimum exploration at .1\n","                print(\"New goal exploration is %s\"%(self.ep2))\n","        #env.close()\n","        #utils.show_video()\n","\n","    def update_Q1(self, states, actions, next_states, rewards):\n","        preds = []\n","        expect = []\n","        C_pred.zero_grad()\n","        for i, _ in enumerate(states):\n","            #C_pred.zero_grad()\n","            prediction = C_pred(states[i].to(device))\n","            prediction = prediction[actions[i]]\n","            preds.append(prediction)\n","            expectation = C_targ(next_states[i])\n","            expectation = rewards[i] + (self.gamma * torch.max(expectation, dim=0)[0]) #notice, as long as (gamma^t_prime-t) is relative to the previous state it's gamma^1 = gamma\n","            expect.append(expectation)\n","            # loss = F.mse_loss(prediction, expectation.detach())\n","            # loss.backward()\n","            # optimizerC.step()\n","        prediction = torch.stack(preds, 0)\n","        expectation = torch.stack(expect, 0)\n","        loss = F.mse_loss(prediction, expectation.detach())\n","        loss.backward()\n","        optimizerC.step()\n","\n","    def update_Q2(self, states, _goals, next_states, rewards):\n","        preds = []\n","        expect = []\n","        C_pred.zero_grad()\n","        for i, _ in enumerate(states):\n","            #MC_pred.zero_grad()\n","            prediction = MC_pred(states[i].to(device))\n","            prediction = prediction[_goals[i]]\n","            preds.append(prediction)\n","            expectation = MC_targ(next_states[i])\n","            expectation = rewards[i] + (gamma * torch.max(expectation, dim=0)[0]) #notice, as long as (gamma^t_prime-t) is relative to the previous state it's gamma^1 = gamma\n","            expect.append(expectation)\n","            # loss = F.mse_loss(prediction, expectation.detach())\n","            # loss.backward()\n","            # optimizerMC.step()\n","        prediction = torch.stack(preds, 0)\n","        expectation = torch.stack(expect, 0)\n","        loss = F.mse_loss(prediction, expectation.detach())\n","        loss.backward()\n","        optimizerC.step()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQLc4VcEQW4q","colab_type":"code","outputId":"33af0a2a-7f54-49f2-839e-5fca7dd2fa7b","executionInfo":{"status":"ok","timestamp":1570483238400,"user_tz":300,"elapsed":60432,"user":{"displayName":"Pensive","photoUrl":"","userId":"04437951639433759578"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["!wget -O utils.py https://drive.google.com/uc?id=18arMwKmz1KUpNxxD-pMPDtWSm8I2OG0i\n","import utils"],"execution_count":9,"outputs":[{"output_type":"stream","text":["--2019-10-07 21:20:35--  https://drive.google.com/uc?id=18arMwKmz1KUpNxxD-pMPDtWSm8I2OG0i\n","Resolving drive.google.com (drive.google.com)... 74.125.20.101, 74.125.20.138, 74.125.20.102, ...\n","Connecting to drive.google.com (drive.google.com)|74.125.20.101|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-10-0k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vhb860mcvnsotai6r250j28tmt22gjt7/1570478400000/12953947379101213852/*/18arMwKmz1KUpNxxD-pMPDtWSm8I2OG0i [following]\n","Warning: wildcards not supported in HTTP.\n","--2019-10-07 21:20:36--  https://doc-10-0k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/vhb860mcvnsotai6r250j28tmt22gjt7/1570478400000/12953947379101213852/*/18arMwKmz1KUpNxxD-pMPDtWSm8I2OG0i\n","Resolving doc-10-0k-docs.googleusercontent.com (doc-10-0k-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n","Connecting to doc-10-0k-docs.googleusercontent.com (doc-10-0k-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2464 (2.4K) [application/x-python]\n","Saving to: ‘utils.py’\n","\n","utils.py            100%[===================>]   2.41K  --.-KB/s    in 0s      \n","\n","2019-10-07 21:20:36 (111 MB/s) - ‘utils.py’ saved [2464/2464]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vT80DBghrBXf","colab_type":"code","colab":{}},"source":["# env = utils.wrap_env(gym.make('MontezumaRevenge-v0'))\n","# env.reset()\n","# for _ in range(1000):\n","#     env.render(mode='ansi')\n","#     env.step(env.action_space.sample()) # take a random action\n","# env.close()\n","# utils.show_video()\n","# #env.action_space.n\n","# #env.pos\n","# #env.reset()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i7qDgZgoN60S","colab_type":"code","colab":{}},"source":["# environment\n","#env = gym.make('MontezumaRevenge-v4')\n","env = utils.wrap_env(gym.make('MontezumaRevenge-v4'))\n","env.reset()\n","\n","#---memory---\n","# meta-controller\n","D2 = namedtuple('D2',\n","                    ('state', 'goal', 'reward', 'next_state')) #reward = extrinsic reward\n","# controller\n","D1 = namedtuple('D1',\n","                    ('state', 'action', 'reward', 'next_state')) #reward = intrinsic reward\n","\n","#---params---\n","batch_size = 512\n","conv_stack = 'stacked' # 'stacked' or 'side_by_side' but anything other than 'stacker' will trigger 'side_by_side'\n","\n","# meta-controller dims\n","if conv_stack=='stacked':\n","    MC_conv_out = 4 * 64 * 7 * 7 # for 4, 1, 84, 84\n","else:\n","    MC_conv_out = 1 * 64 * 7 * 38 # for 1, 1, 84, 336\n","MC_hidden_size = 512\n","MC_number_of_goals = 6 #num_of_goals\n","\n","# controller dims\n","if conv_stack=='stacked':\n","    C_conv_out = 4 * 64 * 7 * 7 # for 4, 1, 84, 84\n","else:\n","    C_conv_out = 1 * 64 * 7 * 38 # for 1, 1, 84, 336\n","C_hidden_size = 512\n","C_number_of_actions = env.action_space.n #num_of_actions\n","\n","# Agent vars\n","env = env\n","MC_buffer = ReplayMemory(1000000)\n","#MC_buffer = ReplayMemory(50000)\n","C_buffer = ReplayMemory(50000)\n","#C_buffer = ReplayMemory(10000)\n","learning_rate =  0.00025\n","gamma = .99\n","exploration_param2 = 1 #start at 1 go to 0.1\n","exploration_param1 = 1 #start at 1 go to 0.1\n","num_of_goals = 6 #randomly picked 6 ftm #object_detector.goals.n #pseudo\n","num_of_actions = env.action_space.n\n","max_goals_to_try = 2000000\n","#batch_size = 512\n","extrinsic_tries_before_eval = batch_size #setting for batch_size ftm\n","\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","\n","# Assign models\n","MC_pred = Meta_Controller(MC_conv_out, MC_hidden_size, MC_number_of_goals, ngpu).to(device)\n","MC_pred.apply(weights_init)\n","MC_targ = Meta_Controller(MC_conv_out, MC_hidden_size, MC_number_of_goals, ngpu).to(device)\n","MC_targ.apply(weights_init)\n","C_pred = Controller(C_conv_out, C_hidden_size, C_number_of_actions, ngpu).to(device)\n","C_pred.apply(weights_init)\n","C_targ = Controller(C_conv_out, C_hidden_size, C_number_of_actions, ngpu).to(device)\n","C_targ.apply(weights_init)\n","\n","# Handle multi-gpu if desired\n","model = None #not Implemented\n","if (device.type == 'cuda') and (ngpu > 1):\n","    model = nn.DataParallel(model, list(range(ngpu)))\n","\n","# Setup Adam optimizers for MC_pred and C_pred, excluded _targ in attempt to hold those constant\n","optimizerMC = optim.Adam(MC_pred.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n","optimizerC = optim.Adam(C_pred.parameters(), lr=learning_rate, betas=(0.5, 0.999))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aqNsklpHN7Q9","colab_type":"code","outputId":"332ce72a-67fe-4b76-f035-f7e47a699d28","executionInfo":{"status":"error","timestamp":1570483144972,"user_tz":300,"elapsed":5054,"user":{"displayName":"Pensive","photoUrl":"","userId":"04437951639433759578"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#--Agent call--\n","agent = Agent(env, #environment\n","      MC_buffer, #MC_buffer\n","      C_buffer, #C_buffer\n","      learning_rate, #learning_rate\n","      gamma, #gamma\n","      exploration_param1, #exploration_param1\n","      exploration_param2, #exploration_param2\n","      max_goals_to_try, #max_goals_to_try\n","      batch_size, #batch_size\n","      extrinsic_tries_before_eval) #extrinsic_tries_before_eval\n","\n","agent.train()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["action space:  Discrete(18)\n","observation space:  Box(210, 160, 3)\n","reward0:  0.0\n","terminal:  False\n","info:  {'ale.lives': 6}\n","Before processing: (210, 160, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZJbIQWSWyEFkluj/Acsa5mV1ojk/AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["After processing: (1, 84, 84)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["new observation space:  Box(210, 160, 3)\n","----done----\n","self.total_sessions:  1\n","Session ending score:  -58.0\n","Goal names:  ['key', 'middle_ladder', 'left_ladder', 'right_ladder', 'left_door', 'right_door']\n","Goal selection percents:  [0.02730045 0.54834411 0.20172453 0.07421031 0.07421031 0.07421031]\n","Success in this video are:  ['middle_ladder', 'middle_ladder']\n","reduce by:  0.01\n","New action exploration is 0.99\n","------------\n","----done----\n","self.total_sessions:  2\n","Session ending score:  -60.0\n","Goal names:  ['key', 'middle_ladder', 'left_ladder', 'right_ladder', 'left_door', 'right_door']\n","Goal selection percents:  [0.00712306 0.14307041 0.3889057  0.01936247 0.05263266 0.3889057 ]\n","New action exploration is 0.99\n","------------\n","----done----\n","self.total_sessions:  3\n","Session ending score:  -59.0\n","Goal names:  ['key', 'middle_ladder', 'left_ladder', 'right_ladder', 'left_door', 'right_door']\n","Goal selection percents:  [0.00426978 0.23312201 0.08576079 0.01160646 0.03154963 0.63369132]\n","Success in this video are:  ['middle_ladder']\n","reduce by:  0.01\n","New action exploration is 0.98\n","------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BS41LTphlyGa","colab_type":"code","colab":{}},"source":["# env = utils.wrap_env(env)\n","# observation = env.reset()\n","# total_raward_rnd = 0\n","# softmax = lambda x: (np.exp(x - np.amax(x))) / np.sum(np.exp(x - np.amax(x)))\n","# while True:\n","#     env.render()\n","#     policy = softmax(agent.C_pred(observation))\n","#     action = np.random.choice(agent.actions, p=policy)\n","#     observation, reward, done, info = env.step(action)\n","#     total_raward_rnd += reward\n","#     if done: \n","#       break;\n","# print(total_raward_rnd)\n","# env.close()\n","# utils.show_video() "],"execution_count":0,"outputs":[]}]}